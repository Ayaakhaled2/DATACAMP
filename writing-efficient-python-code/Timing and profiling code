𝐄𝐱𝐚𝐦𝐢𝐧𝐢𝐧𝐠 𝐫𝐮𝐧𝐭𝐢𝐦𝐞
------------------
- comparing run time between two line of code that perform the same thing allow us to pick the code with the optimal performance 
𝐅𝐚𝐬𝐭𝐞𝐬𝐭 𝐜𝐨𝐝𝐞 == 𝐦𝐨𝐫𝐞 𝐞𝐟𝐟𝐢𝐜𝐢𝐞𝐧𝐭


(%timeit + code )

>> 102 ms ± 7.48 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

- timeit return statistics about the execution time 
- timeit : runs through the provided code multiple times to estimate the code's execution time 
- num or runs represents how many iterations you would like to use to estimate the runtime 
- num of codes : represents how many times you would like the code to be executed per run 
- you can sepcify the num of runs using flag -r , and num of loops using -n 

%timeit -r2 -n10 num = np.random.int(50000) 
 - %runit : can run on single or multiple lines of code 
 
 
 #Time it with single line of code  :       %timeit + code 
 #timeit with multiple code :             use %%timeit 
 %%timeit 
 x = --- 
 y = ---- 
 
 #save the output of runtime using o flag 

times = %timeit -o x = np.random.rand(100)

- when you save the time for each run this allow you to 
1) see the time of each run 
times.timings

2) the best runtime 

times.best

3) worest time for all run 

times.worst


# you can use the timeit to compare the run time of creating python data structure using 𝐟𝐨𝐫𝐦𝐚𝐥 𝐧𝐚𝐦𝐞 or 𝐥𝐢𝐭𝐞𝐫𝐧𝐚𝐥 𝐬𝐲𝐧𝐭𝐚𝐱
𝐟𝐨𝐫𝐦𝐚𝐥 𝐧𝐚𝐦𝐞 :
alist = list( ) 
asict = dict( ) 
atup = tuple( ) 

𝐥𝐢𝐭𝐞𝐫𝐧𝐚𝐥 𝐬𝐲𝐧𝐭𝐚𝐱:

alist = [] 
adict = {} 
atup = ()


t1 = %timeit -o alist = [ input ] 
t2 = %timeit -o alist = list( input ) 


#literal sytax is faster 




Using %timeit: formal name or literal syntax
Python allows you to create data structures using either a formal name or a literal syntax. In this exercise, 
you'll explore how using a literal syntax for creating a data structure can speed up runtimes.

data structure	formal name	literal syntax
list	list()	[]
dictionary	dict()	{}
tuple	tuple()	()


 Using Python's literal syntax to define a data structure can speed up your runtime.
 Consider using the literal syntaxes (like [] instead of list(), {} instead of dict(), or () instead of tuple()), where applicable, to gain some speed.
 
 
 
 ---------------------------------------------------------------------------------------------------------------------------------------
 
 𝐜𝐨𝐝𝐞 𝐩𝐫𝐨𝐟𝐢𝐥𝐢𝐧𝐠 : allow you to analyze the code more efficiently 
 it is a technique to describe how long and how often varous parts of the program are executed 
 
 pip install line_profiler
 %load_ext line_profiler
 
 %lprun -f function_name function_call(arguments) 
 
 -f : flag used to indicate that we want to calculate the execution time of a func 
-------------------------------------------------------------------------------------------------------------------------------
memory consumption

- import sys : this module contains system specific functions 


sys.getsizeof( obj ) : this return the size of obj 
- this nly give you the size of individula obkect 


Code profiling : to analyze the memeory allocations for each line of code 


pip install memory_profiler 
%load_ext memory_profiler 
%mprun -f fun_name func_call(args)

- functions must be defined in a file and imported 
%mp can be run on functions that defined in physical files not ipython session 

- the result will be in Mib : we can say it is similar to mega bytes 
- small amount of data may result in 0.0 mib ( mib : mebi bytes ) 


Notes :

line : indicate the line num of code 
mem usage : memry used after the exection of this line of code 
increment : shows the difference in memory between current line with respect to previous one 
and this show us the impact of the current line on the total memory usage 
line content : refers to source code that 'd been profiled 

%mprun return the result by querying the operation system and so that he output may differ from the amount of memeory that's actually used by python interpreter 


-f : flag indicate that we are calculate the memeory size of the function 


Example with numpy : By implementing an array approach, you were able to shave off a few MiBs. 
Although this isn't a colossal gain, it still decreases your code's overhead. 
If your input data grows over time, these small improvements could add up to some major efficiency gains.
