EXPLORATORY DATA ANALYSIS ( EDA ) 
the process of organize , plotting and summarizing your data 

GRAPHICAL DATA ANALYSIS : 
take the data in tabular form and represent it graphically 
The greatest value of a picture is that it forces us to notice what we never expected to see.
It is important to understand what you can do before you learn how to measure how well you seem to have done it.
 
 Tnum of bins by default = 10 bins in histogram
 The "square root rule" is a commonly-used rule of thumb for choosing the number of bins: choose the number of bins to be the square root of the number of samples.
 
 
Histogram : 
 different data sets looks different depends on the number of bins 
2) we are not plotting all the data so we are losing the actual values 
 
to solve the dis adv of histogram : 
use be swarm plot 
y-axis : quantitative data 
 
there's a limit to the efficiency of bee swarm plot 
with a big data set : the edges will overlapp 
 
 
# Plot all ECDFs on the same plot
_ = plt.plot(x_set , y_set)
_ = plt.plot(x_vers,y_vers)
_ = plt.plot(x_virg , y_virg)
 
 
 
# Annotate the plot
plt.legend(('setosa', 'versicolor', 'virginica'), loc='lower right')
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('ECDF')
 
# Display the plot
plt.show()
 
 
-------------------------------------------------------------------------------------------------------------------

Numerical summaries 

1) mean : the sum of all data / num of data points 

import numpy as np 
np.mean(data) 

- main problem with mean is that it heavily effected by outliers 
- those data points whose value is far greater or less than most of the rest of the data 


2) median : the middle value of the data set 
- median : 50 percentile ( 50 % ) 
- 50% of the data less than the median value and 50 % is higher 
np.median(data) 

1) sort the data 
2) choose the data in the middle 
------------------------------------------------------
3) PERCENTILE 

np.percentile(df['column'] , [ 25 , 50 . 75 ] ) 
- pass a list of percentile you want 
- percentile not a fraction (2.5th, 25th, 50th, 75th, and 97.5th. >> 2.5 , 25 , 50 , 75 , 97.5) ,  a list containing these ints/floats
- and it returns that data that match this percentile 


4) BOXPLOT 
represent the dataset based on percentiles 

- centre of the boxplot : 50 percentile ( median ) 
- the edges of the boxplot : 25% , 75% 
- the total hight of the box : contains the middle 50% of the data ( 75 - 25 = 50 % ) : called interquartile rane ( IQR ) 
- whiskers : 1.5 IQR ( UP ) 
- any point outside the whiskers : plotting as individual point ( OUTLIERS ) 
- begin 2IQR is outlier 

---------------------------------------

# Numerical summaries 

#Mean 
import numpy as np 
np.mean(value)

#Median 
np.median(data)

#percentile 
np.percentile(df['column'] , [25 , 50 , 75 ]) #list of percentile we want

#Boxplot : Represent the dataset based on the percentiles 
sns.boxplot( x = 'value' , y = 'value' , data = df )
plt.xlabel('text')
plt.ylabel('text')
plt.show()
because it derived from the ranking of sorted data and not on the value of the data the median is immune to the data that takes extreme values 


Boxplot : 
1) Center of the box : median ( 50 % ) 
2) edges of the box ( 25 % and 75 % ) 
3) Hight of the box is : iQR interquartile edge contains 50% of the data ( 75 - 25 = 50 % ) 
4) Whiskers : extend up/down 1.5 IQR
4) being more than 2 IQR from median is outliers : plotted as individuals points 



Vairance : avg the squared distance from the mean 

variance = np.var(df['column'])

1) calculate the distance from the mean ( xi - x` ) 
2) take the avg of all those values 

Computing the variance
It is important to have some understanding of what commonly-used functions are doing under the hood

# Array of differences to mean: differences

differences = versicolor_petal_length - np.mean(versicolor_petal_length)

# Square the differences: diff_sq
 
diff_sq = differences ** 2 

# Compute the mean square difference: variance_explicit

variance_explicit = np.mean(diff_sq)
# Compute the variance using NumPy: variance_np
variance_np = np.var(versicolor_petal_length)

# Print the results

print(variance_np)

# Compute the variance: variance
variance = np.var(versicolor_petal_length)

# Print the square root of the variance
print( np.sqrt(variance))

# Print the standard deviation
print(np.std(versicolor_petal_length))

------------------------------------------------------------------------------------------------------
ğ’ğœğšğ­ğ­ğğ«ğ©ğ¥ğ¨ğ­ : ğ©ğ¥ğ¨ğ­ ğğšğ­ğš ğšğ¬ ğ©ğ¨ğ¢ğ§ğ­ğ¬

Generate a scatter plot 

plt.plot( x_data , y_data , marker = '.' , linestyle = 'None')

-- The numerical summary statistics that go along with the scatterplot : covariance 
-- ğ‚ğ¨ğ¯ğšğ«ğ¢ğšğ§ğœğ : a measuere of how two quantites vary together 

ğ‚ğ¨ğ¯ğšğ«ğ¢ğšğ§ğœğ : from ( -1 to 1) 

-1 : complete anticorrealtion ( negative correlation ) 
1  : complete correlation 
0  : no correlation 

Computing the covariance
The covariance may be computed using the Numpy function np.cov(). For example, we have two sets of data x and y, np.cov(x, y) 
returns a 2D array where entries [0,1] and [1,0] are the covariances. Entry [0,0] is the variance of the data in x, and entry [1,1] is the variance of the data in y. 


Computing the Pearson correlation coefficient
As mentioned in the video, the Pearson correlation coefficient, also called the Pearson r, 
is often easier to interpret than the covariance. It is computed using the np.corrcoef() function. Like np.cov(), it takes two arrays as arguments and returns a 2D array. Entries [0,0] and [1,1] are necessarily equal to 1 (can you think about why?), 
and the value we are after is entry [0,1].
This 2D output array is called the covariance matrix, since it organizes the self- and covariance.
